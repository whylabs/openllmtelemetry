{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "First make sure you have the openllmtelemetry back with the bedrock extras specified. You can use the pip command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openllmtelemetry[bedrock]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring your API Key\n",
    "\n",
    "`openllmtelemetry` supports instrumenting calls to invoke models using the boto3 client's bedrock-runtime and interaction with LLMs such as Titan. With opentelemetry instrumentation you gather and send these to WhyLabs, and for this we need to set your WhyLabs API key, as well as the model you want to send the traces to in WhyLabs platform.\n",
    "\n",
    "Note: The example assumes you boto3 credentials are working to be able to invoke bedrock-runtime models, for details see [boto3 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)\n",
    "\n",
    "In order to send the traces to whylabs you need to configure your model-id and WhyLabs API keys first. If you also want to onboard with WhyLabs Guardrail API, see further details on how to do that [here](https://docs.whylabs.ai/docs/secure/guardrails-api), and after you setup your endpoint you can use that in the `WHYLABS_GUARD_ENDPOINT` below. You can skip that part for now to see straghtforward LLM tracing in WhyLabs platform, but you will get better insights and security by using the WhyLabs Guardrail API as that will add additional metrics to your traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WHYLABS_API_KEY\"] = \"replace-with-whylabs-api-key\"\n",
    "os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = \"replace-with-model-id\" #  e.g. \"model-1\"\n",
    "os.environ[\"WHYLABS_GUARD_ENDPOINT\"] = \"\"\n",
    "os.environ[\"WHYLABS_GUARD_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrumenting your LLM Application\n",
    "\n",
    "You can use the following to instrument your python application that interacts with bedrock-runtime models. After this call, interactions that invoke models with boto3 client will automatically gather open telemetry traces for your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openllmtelemetry\n",
    "\n",
    "tracer = openllmtelemetry.instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your LLM calls to Bedrock\n",
    "\n",
    "Here is a minimal example of invoking a bedrock-runtime model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputTextTokenCount': 12, 'results': [{'tokenCount': 58, 'outputText': \"\\nAmazon Bedrock is a managed service that makes foundation models from leading AI startup and Amazon's own Titan models available through APIs. For up-to-date information on Amazon Bedrock and how 3P models are approved, endorsed or selected please see the provided documentation and relevant FAQs.\", 'completionReason': 'FINISH'}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from logging import getLogger\n",
    "\n",
    "import boto3\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "def bedrock_titan(prompt: str):\n",
    "    response_body = None\n",
    "    try:\n",
    "        model_id = 'amazon.titan-text-express-v1'\n",
    "        brt = boto3.client(service_name='bedrock-runtime')\n",
    "        response = brt.invoke_model(body=json.dumps({\"inputText\": prompt}), modelId=model_id)\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    except Exception as error:\n",
    "        logger.error(f\"A client error occurred:{error}\")\n",
    "\n",
    "    return response_body\n",
    "\n",
    "response = bedrock_titan(\"What are the different version of Titan on AWS Bedrock?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
